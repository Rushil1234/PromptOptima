LLM Optimizer
An advanced token compression system for Large Language Models. This tool can achieve up to 95% token reduction while maintaining high semantic accuracy.

‚ú® Features
Multi-Strategy Compression: Utilizes several methods like LLMLingua, SynthLang, Hybrid, and Ultra for optimal token reduction.

Smart Language Routing: Automatically optimizes prompts by routing them to more token-efficient languages (e.g., Chinese, Korean, Spanish) based on the task.

AI Chat (Optimus): An interactive chatbot demonstrating real-time compression and analytics.

Analytics Dashboard: A clean interface to track performance, compression ratios, and token savings.

Modern UI: A responsive, glassmorphic design built with smooth animations.

üõ†Ô∏è Tech Stack
Framework: Next.js 14

Language: TypeScript

Styling: Tailwind CSS

Animation: Framer Motion

AI: Google Gemini 1.5 Flash

üöÄ Quick Start
Clone the repository:

git clone [https://github.com/Rushil1234/harvardhackdesgin1.git](https://github.com/Rushil1234/harvardhackdesgin1.git)
cd tuff

Install dependencies:

npm install

Set up environment variables:

cp .env.example .env

Add your Google Gemini API key to the newly created .env file.

Run the development server:

npm run dev

Open your browser and navigate to http://localhost:3001.

üìä Performance
Strategy

Avg. Compression

Speed

LLMLingua

68%

Fast

SynthLang

85%

Very Fast

Ultra

92%

Fast

üìù License
This project is licensed under the MIT License.